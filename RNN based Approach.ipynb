{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f87c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#scaler = StandardScaler()\n",
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44065ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "dataset = 'train_FD002.csv'\n",
    "df = pd.read_csv(dir_path + r'/' + dataset, sep=\",\", skipinitialspace=True).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c4f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable the Warning from Tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75236345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df[df.columns[[0, 1]]]\n",
    "df_W = df[df.columns[[2, 3, 4]]]\n",
    "df_S = df[df.columns[list(range(5, 26))]]\n",
    "df_X = pd.concat([df_W, df_S], axis=1)\n",
    "\n",
    "df_X = minmax.fit_transform(df_X)\n",
    "\n",
    "engine_unit = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9766b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        #y = data[i+seq_length, -1]  # Assuming the last column is the target (RUL)\n",
    "        xs.append(x)\n",
    "        #ys.append(y)\n",
    "    return np.array(xs)\n",
    "\n",
    "seq_length = 3  #sequence length\n",
    "X = create_sequences(df_X, seq_length)\n",
    "\n",
    "# Expand dimensions to match the input shape of Conv1D\n",
    "X = np.expand_dims(X, axis=2)\n",
    "df_X = np.reshape(X, (X.shape[0], X.shape[1],24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acf2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Failure penalty\n",
    "c_f = -100\n",
    "#Repair penalty\n",
    "c_r = -50\n",
    "do_nothing = 0\n",
    "policy = {}\n",
    "policy_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581c720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, is_training=True, verbose=True):\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(df_X.shape[1],))\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.reward = 0\n",
    "        self.cycle = 0\n",
    "        self.done = False\n",
    "        self.engine_unit = engine_unit\n",
    "        self.engine_df_A = df_A[df_A['unit'] == self.engine_unit]\n",
    "        self.X = df_X[self.engine_df_A.index[0]:self.engine_df_A.index[-1] + 1 ]\n",
    "        self.state = self.X[self.cycle]\n",
    "        self.failure_state = self.engine_df_A['cycle'].max() - 1\n",
    "        self.train = is_training\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_next_engine_data(self):\n",
    "        self.engine_unit += 1\n",
    "\n",
    "        if self.train:\n",
    "            #training\n",
    "            if self.engine_unit > int((df_A['unit'].max() * 80 / 100)):\n",
    "                self.engine_unit = 1\n",
    "        else:\n",
    "            #testing\n",
    "            if self.engine_unit > df_A['unit'].max():\n",
    "                self.engine_unit = int((df_A['unit'].max() * 80 / 100) + 1)\n",
    "        if self.verbose:\n",
    "            print(\"********|engine unit|********:\", self.engine_unit)\n",
    "\n",
    "        self.engine_df_A = df_A[df_A['unit'] == self.engine_unit]\n",
    "        self.X = df_X[self.engine_df_A.index[0]:self.engine_df_A.index[-1]+ 1]\n",
    "        self.failure_state = self.engine_df_A['cycle'].max() - 1\n",
    "        return self.X\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action 0 represents do nothing\n",
    "        if action == 0:\n",
    "            #if self.verbose:\n",
    "            #    print(\"|hold|:\", self.cycle)\n",
    "            if self.cycle == self.failure_state:\n",
    "                self.reward = (c_f)\n",
    "                self.state = self.X[self.cycle]\n",
    "                self.done = True\n",
    "                if self.train: # training policy\n",
    "                    if (self.engine_unit not in policy or (self.cycle > policy[self.engine_unit]['replace_state'] and self.cycle != self.failure_state)):\n",
    "                        policy[self.engine_unit] = {'unit': self.engine_unit,\n",
    "                                                    'failure_state': self.failure_state,\n",
    "                                                    'replace_state': self.cycle}\n",
    "                else: # testing policy\n",
    "                    policy_test[self.engine_unit] = {'unit': self.engine_unit,\n",
    "                                                     'failure_state': self.failure_state,\n",
    "                                                     'replace_state': self.cycle}\n",
    "                if self.verbose:\n",
    "                    print(\"|cycle reached failure state|:\", self.cycle, \"reward:\", self.reward, '\\n')\n",
    "            else:\n",
    "                self.reward = do_nothing\n",
    "                self.cycle += 1\n",
    "                self.state = self.X[self.cycle]\n",
    "                self.done = False\n",
    "                #if self.verbose:\n",
    "                #    print(\"|system running|\", \"reward:\", self.reward, '\\n')\n",
    "        \n",
    "        #Action 1 represents repair\n",
    "        elif action == 1:\n",
    "            if self.verbose:\n",
    "                print(\"|replace|:\", self.cycle)\n",
    "\n",
    "            # failure (fail penalty and repair penalty)\n",
    "            if self.cycle == self.failure_state:\n",
    "                self.reward = (c_f)\n",
    "            elif self.cycle == 0:\n",
    "                self.reward = (c_r)*2\n",
    "            # replace penalty\n",
    "            else:\n",
    "                self.reward =c_r * (1-(self.cycle / self.failure_state))\n",
    "                #c_r / (self.cycle + 0.1)\n",
    "\n",
    "            self.state = self.X[self.cycle]\n",
    "\n",
    "            #Traning policy building\n",
    "            if self.train:\n",
    "                if (self.engine_unit not in policy or (self.cycle > policy[self.engine_unit]['replace_state'] and self.cycle < self.failure_state)):\n",
    "                    policy[self.engine_unit] = {'unit': self.engine_unit,\n",
    "                                                'failure_state': self.failure_state,\n",
    "                                                'replace_state': self.cycle}\n",
    "            # Testing policy building\n",
    "            else:\n",
    "                policy_test[self.engine_unit] = {'unit': self.engine_unit,\n",
    "                                                 'failure_state': self.failure_state,\n",
    "                                                 'replace_state': self.cycle}\n",
    "            self.done = True\n",
    "        #if self.verbose:\n",
    "        #    print(\"reward:\", self.reward, '\\n')\n",
    "        info = {self.cycle}\n",
    "        return self.state, self.reward, self.done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = self.get_next_engine_data()\n",
    "        self.cycle = 0\n",
    "        self.state = self.X[self.cycle]\n",
    "        self.done = False\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce3da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()\n",
    "n_actions = env.action_space.n          # Num of actions\n",
    "state_dim = env.observation_space.shape # Input shape/state dimensions\n",
    "#n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60078e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.5\n",
    "batch = 3\n",
    "state = env.state\n",
    "epoch=0\n",
    "num_episodes = 10 #int((df_A['unit'].max() * 80 / 100))\n",
    "replay = deque(maxlen=100)\n",
    "initial_epsilon = 0.029\n",
    "epsilon_decay =0.975\n",
    "epsilon=initial_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70300d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,750</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │           \u001b[38;5;34m3,750\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │           \u001b[38;5;34m5,050\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m102\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,902</span> (34.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,902\u001b[0m (34.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,902</span> (34.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,902\u001b[0m (34.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "\n",
    "network = keras.Sequential([\n",
    "    keras.Input(shape = (X.shape[1],24)),\n",
    "    keras.layers.SimpleRNN(50, activation='relu',return_sequences = True),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.SimpleRNN(50),\n",
    "    keras.layers.Dense(n_actions)\n",
    "])\n",
    "\n",
    "network.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4900320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state,epsilon):\n",
    "    value_fn = network.predict(np.array([state]),verbose=0)[0]\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        action =np.random.choice(n_actions, 1)[0] \n",
    "        \n",
    "    else:\n",
    "        action = np.argmax(value_fn)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1bdc258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 2\n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 3\n",
      "|replace|: 3\n",
      "Total Reward:  -49.26829268292683 \n",
      "\n",
      "********|engine unit|********: 4\n",
      "Epoch 0, loss 819.656663435263, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 5\n",
      "Epoch 1, loss 7521.530877829903, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 6\n",
      "Epoch 2, loss 4148.818363771338, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 7\n",
      "Epoch 3, loss 6696.459495004999, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 8\n",
      "Epoch 4, loss 6672.436215690156, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 9\n",
      "Epoch 5, loss 6701.977713618718, epsilon 0.029 \n",
      "|replace|: 4\n",
      "Total Reward:  -48.98989898989899 \n",
      "\n",
      "********|engine unit|********: 10\n",
      "Epoch 6, loss 7512.081558391382, epsilon 0.029 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 11\n",
      "Epoch 7, loss 7477.589194999918, epsilon 0.029 \n",
      "|replace|: 5\n",
      "Total Reward:  -49.074074074074076 \n",
      "\n",
      "********|engine unit|********: 12\n",
      "Epoch 8, loss 1606.718442526996, epsilon 0.029 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.596774193548384 \n",
      "\n",
      "********|engine unit|********: 13\n",
      "Epoch 9, loss 6642.0632283654195, epsilon 0.029 \n",
      "|replace|: 15\n",
      "Total Reward:  -46.68141592920354 \n",
      "\n",
      "********|engine unit|********: 14\n",
      "Epoch 10, loss 4128.3794917181085, epsilon 0.029 \n",
      "|replace|: 4\n",
      "Total Reward:  -48.95833333333333 \n",
      "\n",
      "********|engine unit|********: 15\n",
      "Epoch 11, loss 0.04340448430366802, epsilon 0.029 \n",
      "|replace|: 5\n",
      "Total Reward:  -48.79807692307692 \n",
      "\n",
      "********|engine unit|********: 16\n",
      "Epoch 12, loss 0.05888345201311157, epsilon 0.029 \n",
      "|replace|: 4\n",
      "Total Reward:  -48.69281045751634 \n",
      "\n",
      "********|engine unit|********: 17\n",
      "Epoch 13, loss 3339.5664983957436, epsilon 0.029 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.44134078212291 \n",
      "\n",
      "********|engine unit|********: 18\n",
      "Epoch 14, loss 800.6934158901223, epsilon 0.029 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.519230769230774 \n",
      "\n",
      "********|engine unit|********: 19\n",
      "Epoch 15, loss 3338.3450901674587, epsilon 0.029 \n",
      "|replace|: 17\n",
      "Total Reward:  -44.75308641975309 \n",
      "\n",
      "********|engine unit|********: 20\n",
      "Epoch 16, loss 1543.8635983398551, epsilon 0.029 \n",
      "|replace|: 5\n",
      "Total Reward:  -48.36601307189542 \n",
      "\n",
      "********|engine unit|********: 21\n",
      "Epoch 17, loss 3322.237182381057, epsilon 0.029 \n",
      "|replace|: 27\n",
      "Total Reward:  -42.819148936170215 \n",
      "\n",
      "********|engine unit|********: 22\n",
      "Epoch 18, loss 0.0867798546440617, epsilon 0.029 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.47643979057592 \n",
      "\n",
      "********|engine unit|********: 23\n",
      "Epoch 19, loss 0.06450414910262804, epsilon 0.029 \n",
      "|replace|: 21\n",
      "Total Reward:  -45.0 \n",
      "\n",
      "********|engine unit|********: 24\n",
      "Epoch 20, loss 779.8516159082877, epsilon 0.029 \n",
      "|replace|: 25\n",
      "Total Reward:  -43.42105263157895 \n",
      "\n",
      "********|engine unit|********: 25\n",
      "Epoch 21, loss 0.005608802369885313, epsilon 0.028275 \n",
      "|replace|: 10\n",
      "Total Reward:  -47.549019607843135 \n",
      "\n",
      "********|engine unit|********: 26\n",
      "Epoch 22, loss 0.3025092728827606, epsilon 0.028275 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.46236559139785 \n",
      "\n",
      "********|engine unit|********: 27\n",
      "Epoch 23, loss 625.8107742529259, epsilon 0.028275 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 28\n",
      "Epoch 24, loss 0.164708323689917, epsilon 0.028275 \n",
      "|replace|: 10\n",
      "Total Reward:  -47.48743718592965 \n",
      "\n",
      "********|engine unit|********: 29\n",
      "Epoch 25, loss 0.035086166726539934, epsilon 0.028275 \n",
      "|replace|: 12\n",
      "Total Reward:  -46.0 \n",
      "\n",
      "********|engine unit|********: 30\n",
      "Epoch 26, loss 0.06978828361812174, epsilon 0.028275 \n",
      "|replace|: 30\n",
      "Total Reward:  -40.90909090909091 \n",
      "\n",
      "********|engine unit|********: 31\n",
      "Epoch 27, loss 0.0571135005528555, epsilon 0.028275 \n",
      "|replace|: 16\n",
      "Total Reward:  -47.66081871345029 \n",
      "\n",
      "********|engine unit|********: 32\n",
      "Epoch 28, loss 0.071857472857994, epsilon 0.028275 \n",
      "|replace|: 34\n",
      "Total Reward:  -43.92857142857143 \n",
      "\n",
      "********|engine unit|********: 33\n",
      "Epoch 29, loss 752.1240060001284, epsilon 0.028275 \n",
      "|replace|: 31\n",
      "Total Reward:  -42.01030927835052 \n",
      "\n",
      "********|engine unit|********: 34\n",
      "Epoch 30, loss 0.09754798690195894, epsilon 0.028275 \n",
      "|replace|: 9\n",
      "Total Reward:  -47.794117647058826 \n",
      "\n",
      "********|engine unit|********: 35\n",
      "Epoch 31, loss 0.02050352518469843, epsilon 0.028275 \n",
      "|replace|: 29\n",
      "Total Reward:  -42.56410256410256 \n",
      "\n",
      "********|engine unit|********: 36\n",
      "Epoch 32, loss 0.056738493102746874, epsilon 0.028275 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.504950495049506 \n",
      "\n",
      "********|engine unit|********: 37\n",
      "Epoch 33, loss 0.12544523574929856, epsilon 0.028275 \n",
      "|replace|: 3\n",
      "Total Reward:  -49.282296650717704 \n",
      "\n",
      "********|engine unit|********: 38\n",
      "Epoch 34, loss 600.5933684647956, epsilon 0.028275 \n",
      "|replace|: 25\n",
      "Total Reward:  -42.89772727272727 \n",
      "\n",
      "********|engine unit|********: 39\n",
      "Epoch 35, loss 0.0007349342357054015, epsilon 0.028275 \n",
      "|replace|: 21\n",
      "Total Reward:  -45.07042253521127 \n",
      "\n",
      "********|engine unit|********: 40\n",
      "Epoch 36, loss 0.0068043245252393985, epsilon 0.028275 \n",
      "|replace|: 10\n",
      "Total Reward:  -47.39583333333333 \n",
      "\n",
      "********|engine unit|********: 41\n",
      "Epoch 37, loss 0.029157514474948132, epsilon 0.028275 \n",
      "|replace|: 16\n",
      "Total Reward:  -46.934865900383144 \n",
      "\n",
      "********|engine unit|********: 42\n",
      "Epoch 38, loss 0.0031761912357911744, epsilon 0.028275 \n",
      "|replace|: 46\n",
      "Total Reward:  -39.54545454545455 \n",
      "\n",
      "********|engine unit|********: 43\n",
      "Epoch 39, loss 0.028586930193806436, epsilon 0.028275 \n",
      "|replace|: 4\n",
      "Total Reward:  -49.047619047619044 \n",
      "\n",
      "********|engine unit|********: 44\n",
      "Epoch 40, loss 672.6684887228827, epsilon 0.028275 \n",
      "|replace|: 24\n",
      "Total Reward:  -44.11764705882353 \n",
      "\n",
      "********|engine unit|********: 45\n",
      "Epoch 41, loss 0.034137761196269295, epsilon 0.027568125000000002 \n",
      "|replace|: 63\n",
      "Total Reward:  -29.000000000000004 \n",
      "\n",
      "********|engine unit|********: 46\n",
      "Epoch 42, loss 0.0533896160011133, epsilon 0.027568125000000002 \n",
      "|replace|: 110\n",
      "Total Reward:  -22.499999999999996 \n",
      "\n",
      "********|engine unit|********: 47\n",
      "Epoch 43, loss 0.04462701495856871, epsilon 0.027568125000000002 \n",
      "|replace|: 10\n",
      "Total Reward:  -47.95918367346938 \n",
      "\n",
      "********|engine unit|********: 48\n",
      "Epoch 44, loss 0.07388901987684833, epsilon 0.027568125000000002 \n",
      "|replace|: 6\n",
      "Total Reward:  -48.739495798319325 \n",
      "\n",
      "********|engine unit|********: 49\n",
      "Epoch 45, loss 0.09474162566046918, epsilon 0.027568125000000002 \n",
      "|replace|: 33\n",
      "Total Reward:  -40.119760479041915 \n",
      "\n",
      "********|engine unit|********: 50\n",
      "Epoch 46, loss 0.04871960389586092, epsilon 0.027568125000000002 \n",
      "|replace|: 162\n",
      "Total Reward:  -13.348416289592762 \n",
      "\n",
      "********|engine unit|********: 51\n",
      "Epoch 47, loss 0.017337218460211263, epsilon 0.027568125000000002 \n",
      "|replace|: 87\n",
      "Total Reward:  -35.1027397260274 \n",
      "\n",
      "********|engine unit|********: 52\n",
      "Epoch 48, loss 0.030931274817661825, epsilon 0.027568125000000002 \n",
      "|replace|: 18\n",
      "Total Reward:  -44.797687861271676 \n",
      "\n",
      "********|engine unit|********: 53\n",
      "Epoch 49, loss 0.13549096501200134, epsilon 0.027568125000000002 \n",
      "|replace|: 77\n",
      "Total Reward:  -23.809523809523807 \n",
      "\n",
      "********|engine unit|********: 54\n",
      "Epoch 50, loss 0.044207748353299825, epsilon 0.027568125000000002 \n",
      "|replace|: 58\n",
      "Total Reward:  -35.35353535353536 \n",
      "\n",
      "********|engine unit|********: 55\n",
      "Epoch 51, loss 0.0279330591428492, epsilon 0.027568125000000002 \n",
      "|replace|: 15\n",
      "Total Reward:  -45.25316455696202 \n",
      "\n",
      "********|engine unit|********: 56\n",
      "Epoch 52, loss 0.01599716505103874, epsilon 0.027568125000000002 \n",
      "|replace|: 9\n",
      "Total Reward:  -47.22222222222222 \n",
      "\n",
      "********|engine unit|********: 57\n",
      "Epoch 53, loss 0.006174649509107233, epsilon 0.027568125000000002 \n",
      "|replace|: 34\n",
      "Total Reward:  -37.76978417266187 \n",
      "\n",
      "********|engine unit|********: 58\n",
      "Epoch 54, loss 406.3462427860686, epsilon 0.027568125000000002 \n",
      "|replace|: 104\n",
      "Total Reward:  -17.70186335403727 \n",
      "\n",
      "********|engine unit|********: 59\n",
      "Epoch 55, loss 0.0019166226238019855, epsilon 0.027568125000000002 \n",
      "|replace|: 87\n",
      "Total Reward:  -27.461139896373055 \n",
      "\n",
      "********|engine unit|********: 60\n",
      "Epoch 56, loss 0.006033757635601438, epsilon 0.027568125000000002 \n",
      "|replace|: 146\n",
      "Total Reward:  -8.52272727272727 \n",
      "\n",
      "********|engine unit|********: 61\n",
      "Epoch 57, loss 0.023434436758742225, epsilon 0.027568125000000002 \n",
      "|cycle reached failure state|: 217 reward: -100 \n",
      "\n",
      "********|engine unit|********: 62\n",
      "Epoch 58, loss 0.03128685617673618, epsilon 0.027568125000000002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|replace|: 109\n",
      "Total Reward:  -20.21857923497268 \n",
      "\n",
      "********|engine unit|********: 63\n",
      "Epoch 59, loss 0.005957250278226414, epsilon 0.027568125000000002 \n",
      "|replace|: 112\n",
      "Total Reward:  -16.467065868263475 \n",
      "\n",
      "********|engine unit|********: 64\n",
      "Epoch 60, loss 0.03957307527397284, epsilon 0.027568125000000002 \n",
      "|replace|: 81\n",
      "Total Reward:  -35.58718861209964 \n",
      "\n",
      "********|engine unit|********: 65\n",
      "Epoch 61, loss 0.048489504530787576, epsilon 0.026878921875000003 \n",
      "|replace|: 123\n",
      "Total Reward:  -20.0 \n",
      "\n",
      "********|engine unit|********: 66\n",
      "Epoch 62, loss 0.03261864582038666, epsilon 0.026878921875000003 \n",
      "|replace|: 5\n",
      "Total Reward:  -48.5207100591716 \n",
      "\n",
      "********|engine unit|********: 67\n",
      "Epoch 63, loss 0.0007624594015296146, epsilon 0.026878921875000003 \n",
      "|cycle reached failure state|: 144 reward: -100 \n",
      "\n",
      "********|engine unit|********: 68\n",
      "Epoch 64, loss 3368.8552566626427, epsilon 0.026878921875000003 \n",
      "|cycle reached failure state|: 184 reward: -100 \n",
      "\n",
      "********|engine unit|********: 69\n",
      "Epoch 65, loss 0.04593245298650409, epsilon 0.026878921875000003 \n",
      "|replace|: 85\n",
      "Total Reward:  -18.51851851851852 \n",
      "\n",
      "********|engine unit|********: 70\n",
      "Epoch 66, loss 0.034807885743592816, epsilon 0.026878921875000003 \n",
      "|replace|: 58\n",
      "Total Reward:  -31.290322580645157 \n",
      "\n",
      "********|engine unit|********: 71\n",
      "Epoch 67, loss 0.024600299016070287, epsilon 0.026878921875000003 \n",
      "|replace|: 41\n",
      "Total Reward:  -39.37823834196891 \n",
      "\n",
      "********|engine unit|********: 72\n",
      "Epoch 68, loss 0.016510448136862206, epsilon 0.026878921875000003 \n",
      "|replace|: 19\n",
      "Total Reward:  -45.12820512820513 \n",
      "\n",
      "********|engine unit|********: 73\n",
      "Epoch 69, loss 0.024219370484254172, epsilon 0.026878921875000003 \n",
      "|replace|: 40\n",
      "Total Reward:  -41.59663865546218 \n",
      "\n",
      "********|engine unit|********: 74\n",
      "Epoch 70, loss 0.08728768206045341, epsilon 0.026878921875000003 \n",
      "|replace|: 10\n",
      "Total Reward:  -46.81528662420382 \n",
      "\n",
      "********|engine unit|********: 75\n",
      "Epoch 71, loss 0.022760920394144585, epsilon 0.026878921875000003 \n",
      "|replace|: 38\n",
      "Total Reward:  -41.73913043478261 \n",
      "\n",
      "********|engine unit|********: 76\n",
      "Epoch 72, loss 0.03860128835118565, epsilon 0.026878921875000003 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.58847736625515 \n",
      "\n",
      "********|engine unit|********: 77\n",
      "Epoch 73, loss 810.6797965023237, epsilon 0.026878921875000003 \n",
      "|replace|: 12\n",
      "Total Reward:  -46.73913043478261 \n",
      "\n",
      "********|engine unit|********: 78\n",
      "Epoch 74, loss 0.005455358530162731, epsilon 0.026878921875000003 \n",
      "|replace|: 80\n",
      "Total Reward:  -25.0 \n",
      "\n",
      "********|engine unit|********: 79\n",
      "Epoch 75, loss 0.02072230564613757, epsilon 0.026878921875000003 \n",
      "|replace|: 21\n",
      "Total Reward:  -44.166666666666664 \n",
      "\n",
      "********|engine unit|********: 80\n",
      "Epoch 76, loss 0.027466788758083015, epsilon 0.026878921875000003 \n",
      "|replace|: 117\n",
      "Total Reward:  -23.409090909090907 \n",
      "\n",
      "********|engine unit|********: 81\n",
      "Epoch 77, loss 0.0027475269233664668, epsilon 0.026878921875000003 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.65156794425087 \n",
      "\n",
      "********|engine unit|********: 82\n",
      "Epoch 78, loss 0.04493069869852393, epsilon 0.026878921875000003 \n",
      "|cycle reached failure state|: 199 reward: -100 \n",
      "\n",
      "********|engine unit|********: 83\n",
      "Epoch 79, loss 0.004452661377283884, epsilon 0.026878921875000003 \n",
      "|replace|: 166\n",
      "Total Reward:  -0.8875739644970404 \n",
      "\n",
      "********|engine unit|********: 84\n",
      "Epoch 80, loss 0.02969910541938768, epsilon 0.026878921875000003 \n",
      "|replace|: 38\n",
      "Total Reward:  -42.63565891472868 \n",
      "\n",
      "********|engine unit|********: 85\n",
      "Epoch 81, loss 0.003213021297813682, epsilon 0.026206948828125003 \n",
      "|replace|: 153\n",
      "Total Reward:  -27.89017341040463 \n",
      "\n",
      "********|engine unit|********: 86\n",
      "Epoch 82, loss 0.0888754596365679, epsilon 0.026206948828125003 \n",
      "|replace|: 148\n",
      "Total Reward:  -10.846560846560848 \n",
      "\n",
      "********|engine unit|********: 87\n",
      "Epoch 83, loss 0.060946501168528344, epsilon 0.026206948828125003 \n",
      "|cycle reached failure state|: 169 reward: -100 \n",
      "\n",
      "********|engine unit|********: 88\n",
      "Epoch 84, loss 0.031181001284486632, epsilon 0.026206948828125003 \n",
      "|replace|: 4\n",
      "Total Reward:  -49.45054945054945 \n",
      "\n",
      "********|engine unit|********: 89\n",
      "Epoch 85, loss 0.11943931624227294, epsilon 0.026206948828125003 \n",
      "|replace|: 92\n",
      "Total Reward:  -25.40106951871658 \n",
      "\n",
      "********|engine unit|********: 90\n",
      "Epoch 86, loss 0.033428099954831945, epsilon 0.026206948828125003 \n",
      "|replace|: 11\n",
      "Total Reward:  -47.05882352941176 \n",
      "\n",
      "********|engine unit|********: 91\n",
      "Epoch 87, loss 0.05058498824537411, epsilon 0.026206948828125003 \n",
      "|replace|: 0\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 92\n",
      "Epoch 88, loss 0.010244072546353686, epsilon 0.026206948828125003 \n",
      "|replace|: 113\n",
      "Total Reward:  -19.786096256684495 \n",
      "\n",
      "********|engine unit|********: 93\n",
      "Epoch 89, loss 0.042193763295841215, epsilon 0.026206948828125003 \n",
      "|replace|: 6\n",
      "Total Reward:  -48.40425531914894 \n",
      "\n",
      "********|engine unit|********: 94\n",
      "Epoch 90, loss 0.007569038843844181, epsilon 0.026206948828125003 \n",
      "|replace|: 115\n",
      "Total Reward:  -31.147540983606557 \n",
      "\n",
      "********|engine unit|********: 95\n",
      "Epoch 91, loss 310.2216000319547, epsilon 0.026206948828125003 \n",
      "|replace|: 10\n",
      "Total Reward:  -47.46192893401015 \n",
      "\n",
      "********|engine unit|********: 96\n",
      "Epoch 92, loss 0.03262148046097899, epsilon 0.026206948828125003 \n",
      "|replace|: 49\n",
      "Total Reward:  -38.16425120772947 \n",
      "\n",
      "********|engine unit|********: 97\n",
      "Epoch 93, loss 0.002533831185319061, epsilon 0.026206948828125003 \n",
      "|replace|: 137\n",
      "Total Reward:  -2.430555555555558 \n",
      "\n",
      "********|engine unit|********: 98\n",
      "Epoch 94, loss 0.08895505883526444, epsilon 0.026206948828125003 \n",
      "|replace|: 18\n",
      "Total Reward:  -44.57831325301205 \n",
      "\n",
      "********|engine unit|********: 99\n",
      "Epoch 95, loss 0.04841360994446578, epsilon 0.026206948828125003 \n",
      "|replace|: 66\n",
      "Total Reward:  -33.582089552238806 \n",
      "\n",
      "********|engine unit|********: 100\n",
      "Epoch 96, loss 0.0784508287781428, epsilon 0.026206948828125003 \n",
      "|replace|: 74\n",
      "Total Reward:  -32.464454976303315 \n",
      "\n",
      "********|engine unit|********: 101\n",
      "Epoch 97, loss 0.0018755847681879306, epsilon 0.026206948828125003 \n",
      "|cycle reached failure state|: 210 reward: -100 \n",
      "\n",
      "********|engine unit|********: 102\n",
      "Epoch 98, loss 0.04418121420450366, epsilon 0.026206948828125003 \n",
      "|replace|: 62\n",
      "Total Reward:  -30.981595092024538 \n",
      "\n",
      "********|engine unit|********: 103\n",
      "Epoch 99, loss 0.08078218447948125, epsilon 0.026206948828125003 \n",
      "|replace|: 79\n",
      "Total Reward:  -35.842293906810035 \n",
      "\n",
      "********|engine unit|********: 104\n",
      "Epoch 100, loss 0.03302455650941346, epsilon 0.026206948828125003 \n",
      "|replace|: 35\n",
      "Total Reward:  -42.290748898678416 \n",
      "\n",
      "********|engine unit|********: 105\n",
      "Epoch 101, loss 0.03338246179444173, epsilon 0.02555177510742188 \n",
      "|replace|: 63\n",
      "Total Reward:  -39.56953642384106 \n",
      "\n",
      "********|engine unit|********: 106\n",
      "Epoch 102, loss 0.024681712953962733, epsilon 0.02555177510742188 \n",
      "|replace|: 119\n",
      "Total Reward:  -28.044280442804425 \n",
      "\n",
      "********|engine unit|********: 107\n",
      "Epoch 103, loss 0.021297805540425388, epsilon 0.02555177510742188 \n",
      "|replace|: 62\n",
      "Total Reward:  -28.47222222222222 \n",
      "\n",
      "********|engine unit|********: 108\n",
      "Epoch 104, loss 0.03674287800630605, epsilon 0.02555177510742188 \n",
      "|replace|: 32\n",
      "Total Reward:  -39.67741935483871 \n",
      "\n",
      "********|engine unit|********: 109\n",
      "Epoch 105, loss 0.025498561268230895, epsilon 0.02555177510742188 \n",
      "|replace|: 21\n",
      "Total Reward:  -45.205479452054796 \n",
      "\n",
      "********|engine unit|********: 110\n",
      "Epoch 106, loss 0.051956488926316234, epsilon 0.02555177510742188 \n",
      "|replace|: 32\n",
      "Total Reward:  -42.72727272727273 \n",
      "\n",
      "********|engine unit|********: 111\n",
      "Epoch 107, loss 0.06296328569600952, epsilon 0.02555177510742188 \n",
      "|replace|: 22\n",
      "Total Reward:  -44.472361809045225 \n",
      "\n",
      "********|engine unit|********: 112\n",
      "Epoch 108, loss 0.06426509084197367, epsilon 0.02555177510742188 \n",
      "|replace|: 41\n",
      "Total Reward:  -44.56233421750663 \n",
      "\n",
      "********|engine unit|********: 113\n",
      "Epoch 109, loss 0.029175353091247542, epsilon 0.02555177510742188 \n",
      "|replace|: 11\n",
      "Total Reward:  -47.61904761904761 \n",
      "\n",
      "********|engine unit|********: 114\n",
      "Epoch 110, loss 0.03888460324354449, epsilon 0.02555177510742188 \n",
      "|replace|: 150\n",
      "Total Reward:  -12.871287128712872 \n",
      "\n",
      "********|engine unit|********: 115\n",
      "Epoch 111, loss 0.0032782192700768625, epsilon 0.02555177510742188 \n",
      "|replace|: 47\n",
      "Total Reward:  -38.53658536585366 \n",
      "\n",
      "********|engine unit|********: 116\n",
      "Epoch 112, loss 0.003130803722698427, epsilon 0.02555177510742188 \n",
      "|cycle reached failure state|: 206 reward: -100 \n",
      "\n",
      "********|engine unit|********: 117\n",
      "Epoch 113, loss 0.06603555047557252, epsilon 0.02555177510742188 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|cycle reached failure state|: 176 reward: -100 \n",
      "\n",
      "********|engine unit|********: 118\n",
      "Epoch 114, loss 0.017803457478658174, epsilon 0.02555177510742188 \n",
      "|replace|: 178\n",
      "Total Reward:  -24.05247813411079 \n",
      "\n",
      "********|engine unit|********: 119\n",
      "Epoch 115, loss 0.014648599054449193, epsilon 0.02555177510742188 \n",
      "|replace|: 16\n",
      "Total Reward:  -45.3757225433526 \n",
      "\n",
      "********|engine unit|********: 120\n",
      "Epoch 116, loss 0.006421858225046129, epsilon 0.02555177510742188 \n",
      "|replace|: 37\n",
      "Total Reward:  -35.546875 \n",
      "\n",
      "********|engine unit|********: 121\n",
      "Epoch 117, loss 0.010599396238887284, epsilon 0.02555177510742188 \n",
      "|replace|: 30\n",
      "Total Reward:  -42.822966507177036 \n",
      "\n",
      "********|engine unit|********: 122\n",
      "Epoch 118, loss 599.6795669250369, epsilon 0.02555177510742188 \n",
      "|cycle reached failure state|: 184 reward: -100 \n",
      "\n",
      "********|engine unit|********: 123\n",
      "Epoch 119, loss 0.03963327684955125, epsilon 0.02555177510742188 \n",
      "|replace|: 11\n",
      "Total Reward:  -47.55555555555556 \n",
      "\n",
      "********|engine unit|********: 124\n",
      "Epoch 120, loss 0.0289001169699102, epsilon 0.02555177510742188 \n",
      "|replace|: 44\n",
      "Total Reward:  -41.72932330827068 \n",
      "\n",
      "********|engine unit|********: 125\n",
      "Epoch 121, loss 0.0029650193301292795, epsilon 0.02491298072973633 \n",
      "|replace|: 13\n",
      "Total Reward:  -46.10778443113772 \n",
      "\n",
      "********|engine unit|********: 126\n",
      "Epoch 122, loss 0.008811801765880041, epsilon 0.02491298072973633 \n",
      "|replace|: 55\n",
      "Total Reward:  -36.11111111111111 \n",
      "\n",
      "********|engine unit|********: 127\n",
      "Epoch 123, loss 0.018553644815234794, epsilon 0.02491298072973633 \n",
      "|replace|: 11\n",
      "Total Reward:  -47.84313725490196 \n",
      "\n",
      "********|engine unit|********: 128\n",
      "Epoch 124, loss 0.0557782574299436, epsilon 0.02491298072973633 \n",
      "|replace|: 59\n",
      "Total Reward:  -40.2317880794702 \n",
      "\n",
      "********|engine unit|********: 129\n",
      "Epoch 125, loss 0.020269499366835742, epsilon 0.02491298072973633 \n",
      "|replace|: 15\n",
      "Total Reward:  -46.26865671641791 \n",
      "\n",
      "********|engine unit|********: 130\n",
      "Epoch 126, loss 0.05005813473377494, epsilon 0.02491298072973633 \n",
      "|replace|: 5\n",
      "Total Reward:  -48.61878453038674 \n",
      "\n",
      "********|engine unit|********: 131\n",
      "Epoch 127, loss 0.03261613041769754, epsilon 0.02491298072973633 \n",
      "|replace|: 23\n",
      "Total Reward:  -45.878136200716845 \n",
      "\n",
      "********|engine unit|********: 132\n",
      "Epoch 128, loss 0.01870505923859658, epsilon 0.02491298072973633 \n",
      "|replace|: 30\n",
      "Total Reward:  -41.75824175824176 \n",
      "\n",
      "********|engine unit|********: 133\n",
      "Epoch 129, loss 0.05941739167813229, epsilon 0.02491298072973633 \n",
      "|cycle reached failure state|: 158 reward: -100 \n",
      "\n",
      "********|engine unit|********: 134\n",
      "Epoch 130, loss 0.04783427495443091, epsilon 0.02491298072973633 \n",
      "|replace|: 105\n",
      "Total Reward:  -28.65853658536586 \n",
      "\n",
      "********|engine unit|********: 135\n",
      "Epoch 131, loss 0.006061664718699881, epsilon 0.02491298072973633 \n",
      "|replace|: 94\n",
      "Total Reward:  -33.95904436860068 \n",
      "\n",
      "********|engine unit|********: 136\n",
      "Epoch 132, loss 0.0040204511553435995, epsilon 0.02491298072973633 \n",
      "|cycle reached failure state|: 159 reward: -100 \n",
      "\n",
      "********|engine unit|********: 137\n",
      "Epoch 133, loss 0.028903743046825785, epsilon 0.02491298072973633 \n",
      "|replace|: 52\n",
      "Total Reward:  -32.31292517006803 \n",
      "\n",
      "********|engine unit|********: 138\n",
      "Epoch 134, loss 0.02670811651680388, epsilon 0.02491298072973633 \n",
      "|replace|: 24\n",
      "Total Reward:  -43.969849246231156 \n",
      "\n",
      "********|engine unit|********: 139\n",
      "Epoch 135, loss 0.056820540231445144, epsilon 0.02491298072973633 \n",
      "|replace|: 25\n",
      "Total Reward:  -43.523316062176164 \n",
      "\n",
      "********|engine unit|********: 140\n",
      "Epoch 136, loss 0.028660733105634145, epsilon 0.02491298072973633 \n",
      "|replace|: 33\n",
      "Total Reward:  -42.25352112676056 \n",
      "\n",
      "********|engine unit|********: 141\n",
      "Epoch 137, loss 0.017397864478027037, epsilon 0.02491298072973633 \n",
      "|replace|: 74\n",
      "Total Reward:  -27.160493827160494 \n",
      "\n",
      "********|engine unit|********: 142\n",
      "Epoch 138, loss 0.01766084712315767, epsilon 0.02491298072973633 \n",
      "|replace|: 6\n",
      "Total Reward:  -48.55072463768116 \n",
      "\n",
      "********|engine unit|********: 143\n",
      "Epoch 139, loss 0.0160192437731673, epsilon 0.02491298072973633 \n",
      "|replace|: 24\n",
      "Total Reward:  -42.72727272727273 \n",
      "\n",
      "********|engine unit|********: 144\n",
      "Epoch 140, loss 0.004399676297732729, epsilon 0.02491298072973633 \n",
      "|replace|: 85\n",
      "Total Reward:  -30.94170403587444 \n",
      "\n",
      "********|engine unit|********: 145\n",
      "Epoch 141, loss 0.003873787550871819, epsilon 0.024290156211492924 \n",
      "|replace|: 87\n",
      "Total Reward:  -32.6 \n",
      "\n",
      "********|engine unit|********: 146\n",
      "Epoch 142, loss 0.03691563425321054, epsilon 0.024290156211492924 \n",
      "|replace|: 112\n",
      "Total Reward:  -28.62595419847328 \n",
      "\n",
      "********|engine unit|********: 147\n",
      "Epoch 143, loss 0.021480235623770605, epsilon 0.024290156211492924 \n",
      "|replace|: 170\n",
      "Total Reward:  -8.737864077669904 \n",
      "\n",
      "********|engine unit|********: 148\n",
      "Epoch 144, loss 0.01943415837878935, epsilon 0.024290156211492924 \n",
      "|replace|: 177\n",
      "Total Reward:  -18.16546762589928 \n",
      "\n",
      "********|engine unit|********: 149\n",
      "Epoch 145, loss 0.002222322527405351, epsilon 0.024290156211492924 \n",
      "|replace|: 2\n",
      "Total Reward:  -49.65277777777778 \n",
      "\n",
      "********|engine unit|********: 150\n",
      "Epoch 146, loss 0.0072731739393982996, epsilon 0.024290156211492924 \n",
      "|replace|: 7\n",
      "Total Reward:  -48.41628959276018 \n",
      "\n",
      "********|engine unit|********: 151\n",
      "Epoch 147, loss 0.017265782755601616, epsilon 0.024290156211492924 \n",
      "|replace|: 145\n",
      "Total Reward:  -8.806818181818182 \n",
      "\n",
      "********|engine unit|********: 152\n",
      "Epoch 148, loss 0.006008923982391965, epsilon 0.024290156211492924 \n",
      "|replace|: 1\n",
      "Total Reward:  -49.72677595628415 \n",
      "\n",
      "********|engine unit|********: 153\n",
      "Epoch 149, loss 0.02712719113644245, epsilon 0.024290156211492924 \n",
      "|replace|: 119\n",
      "Total Reward:  -29.553264604810998 \n",
      "\n",
      "********|engine unit|********: 154\n",
      "Epoch 150, loss 0.004732089082081282, epsilon 0.024290156211492924 \n",
      "|replace|: 36\n",
      "Total Reward:  -40.625 \n",
      "\n",
      "********|engine unit|********: 155\n",
      "Epoch 151, loss 0.014998944157485058, epsilon 0.024290156211492924 \n",
      "|cycle reached failure state|: 225 reward: -100 \n",
      "\n",
      "********|engine unit|********: 156\n",
      "Epoch 152, loss 0.019762645824661633, epsilon 0.024290156211492924 \n",
      "|replace|: 112\n",
      "Total Reward:  -31.20805369127517 \n",
      "\n",
      "********|engine unit|********: 157\n",
      "Epoch 153, loss 0.010372489874486544, epsilon 0.024290156211492924 \n",
      "|replace|: 50\n",
      "Total Reward:  -39.40677966101695 \n",
      "\n",
      "********|engine unit|********: 158\n",
      "Epoch 154, loss 0.041849956147603735, epsilon 0.024290156211492924 \n",
      "|replace|: 63\n",
      "Total Reward:  -39.73941368078176 \n",
      "\n",
      "********|engine unit|********: 159\n",
      "Epoch 155, loss 483.7453125852763, epsilon 0.024290156211492924 \n",
      "|replace|: 37\n",
      "Total Reward:  -42.291666666666664 \n",
      "\n",
      "********|engine unit|********: 160\n",
      "Epoch 156, loss 0.010042087224591375, epsilon 0.024290156211492924 \n",
      "|replace|: 14\n",
      "Total Reward:  -46.682464454976305 \n",
      "\n",
      "********|engine unit|********: 161\n",
      "Epoch 157, loss 0.08121355065680873, epsilon 0.024290156211492924 \n",
      "|replace|: 8\n",
      "Total Reward:  -48.20627802690583 \n",
      "\n",
      "********|engine unit|********: 162\n",
      "Epoch 158, loss 704.8800008209159, epsilon 0.024290156211492924 \n",
      "|replace|: 15\n",
      "Total Reward:  -46.1139896373057 \n",
      "\n",
      "********|engine unit|********: 163\n",
      "Epoch 159, loss 0.06819279842803311, epsilon 0.024290156211492924 \n",
      "|replace|: 4\n",
      "Total Reward:  -49.00990099009901 \n",
      "\n",
      "********|engine unit|********: 164\n",
      "Epoch 160, loss 0.021529822127833737, epsilon 0.024290156211492924 \n",
      "|replace|: 3\n",
      "Total Reward:  -49.395161290322584 \n",
      "\n",
      "********|engine unit|********: 165\n",
      "Epoch 161, loss 0.004693923384303897, epsilon 0.0236829023062056 \n",
      "|replace|: 1\n",
      "Total Reward:  -49.69512195121951 \n",
      "\n",
      "********|engine unit|********: 166\n",
      "Epoch 162, loss 0.037913703453838134, epsilon 0.0236829023062056 \n",
      "|replace|: 193\n",
      "Total Reward:  -2.463054187192121 \n",
      "\n",
      "********|engine unit|********: 167\n",
      "Epoch 163, loss 0.005050966851107656, epsilon 0.0236829023062056 \n",
      "|cycle reached failure state|: 231 reward: -100 \n",
      "\n",
      "********|engine unit|********: 168\n",
      "Epoch 164, loss 0.02487018969593892, epsilon 0.0236829023062056 \n",
      "|replace|: 12\n",
      "Total Reward:  -47.22222222222222 \n",
      "\n",
      "********|engine unit|********: 169\n",
      "Epoch 165, loss 0.02301633620488014, epsilon 0.0236829023062056 \n",
      "|replace|: 35\n",
      "Total Reward:  -40.4891304347826 \n",
      "\n",
      "********|engine unit|********: 170\n",
      "Epoch 166, loss 0.005512215129803026, epsilon 0.0236829023062056 \n",
      "|cycle reached failure state|: 178 reward: -100 \n",
      "\n",
      "********|engine unit|********: 171\n",
      "Epoch 167, loss 0.027004320683815978, epsilon 0.0236829023062056 \n",
      "|replace|: 168\n",
      "Total Reward:  -17.315175097276263 \n",
      "\n",
      "********|engine unit|********: 172\n",
      "Epoch 168, loss 0.027627696764560578, epsilon 0.0236829023062056 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|replace|: 30\n",
      "Total Reward:  -41.32947976878613 \n",
      "\n",
      "********|engine unit|********: 173\n",
      "Epoch 169, loss 0.06218030949272898, epsilon 0.0236829023062056 \n",
      "|replace|: 51\n",
      "Total Reward:  -32.99999999999999 \n",
      "\n",
      "********|engine unit|********: 174\n",
      "Epoch 170, loss 0.04544091285492263, epsilon 0.0236829023062056 \n",
      "|replace|: 97\n",
      "Total Reward:  -24.739583333333332 \n",
      "\n",
      "********|engine unit|********: 175\n",
      "Epoch 171, loss 0.018254098827099347, epsilon 0.0236829023062056 \n",
      "|replace|: 82\n",
      "Total Reward:  -29.601990049751247 \n",
      "\n",
      "********|engine unit|********: 176\n",
      "Epoch 172, loss 0.015788980084190062, epsilon 0.0236829023062056 \n",
      "|replace|: 153\n",
      "Total Reward:  -3.0674846625766863 \n",
      "\n",
      "********|engine unit|********: 177\n",
      "Epoch 173, loss 0.005149878861844409, epsilon 0.0236829023062056 \n",
      "|replace|: 93\n",
      "Total Reward:  -24.166666666666664 \n",
      "\n",
      "********|engine unit|********: 178\n",
      "Epoch 174, loss 0.03928774603714703, epsilon 0.0236829023062056 \n",
      "|replace|: 95\n",
      "Total Reward:  -17.68707482993197 \n",
      "\n",
      "********|engine unit|********: 179\n",
      "Epoch 175, loss 0.015668994209984792, epsilon 0.0236829023062056 \n",
      "|cycle reached failure state|: 180 reward: -100 \n",
      "\n",
      "********|engine unit|********: 180\n",
      "Epoch 176, loss 0.0124451101348624, epsilon 0.0236829023062056 \n",
      "|replace|: 83\n",
      "Total Reward:  -35.979729729729726 \n",
      "\n",
      "********|engine unit|********: 181\n",
      "Epoch 177, loss 0.009596466768151493, epsilon 0.0236829023062056 \n",
      "|replace|: 137\n",
      "Total Reward:  -23.031496062992122 \n",
      "\n",
      "********|engine unit|********: 182\n",
      "Epoch 178, loss 0.02728699634137859, epsilon 0.0236829023062056 \n",
      "|replace|: 148\n",
      "Total Reward:  -12.814070351758794 \n",
      "\n",
      "********|engine unit|********: 183\n",
      "Epoch 179, loss 0.013652287349172515, epsilon 0.0236829023062056 \n",
      "|replace|: 50\n",
      "Total Reward:  -37.86407766990291 \n",
      "\n",
      "********|engine unit|********: 184\n",
      "Epoch 180, loss 0.015363872566728692, epsilon 0.0236829023062056 \n",
      "|replace|: 59\n",
      "Total Reward:  -32.64705882352941 \n",
      "\n",
      "********|engine unit|********: 185\n",
      "Epoch 181, loss 0.05756683539571154, epsilon 0.02309082974855046 \n",
      "|replace|: 50\n",
      "Total Reward:  -34.472049689441 \n",
      "\n",
      "********|engine unit|********: 186\n",
      "Epoch 182, loss 0.0806961399607057, epsilon 0.02309082974855046 \n",
      "|replace|: 111\n",
      "Total Reward:  -21.243523316062173 \n",
      "\n",
      "********|engine unit|********: 187\n",
      "Epoch 183, loss 0.011247569755493026, epsilon 0.02309082974855046 \n",
      "|cycle reached failure state|: 167 reward: -100 \n",
      "\n",
      "********|engine unit|********: 188\n",
      "Epoch 184, loss 0.018084604334033954, epsilon 0.02309082974855046 \n",
      "|cycle reached failure state|: 147 reward: -100 \n",
      "\n",
      "********|engine unit|********: 189\n",
      "Epoch 185, loss 0.014897743633996421, epsilon 0.02309082974855046 \n",
      "|replace|: 15\n",
      "Total Reward:  -46.394230769230774 \n",
      "\n",
      "********|engine unit|********: 190\n",
      "Epoch 186, loss 0.028601549982382124, epsilon 0.02309082974855046 \n",
      "|replace|: 110\n",
      "Total Reward:  -28.515625 \n",
      "\n",
      "********|engine unit|********: 191\n",
      "Epoch 187, loss 0.011428524560183367, epsilon 0.02309082974855046 \n",
      "|replace|: 169\n",
      "Total Reward:  -11.238532110091743 \n",
      "\n",
      "********|engine unit|********: 192\n",
      "Epoch 188, loss 0.03098205929602504, epsilon 0.02309082974855046 \n",
      "|cycle reached failure state|: 132 reward: -100 \n",
      "\n",
      "********|engine unit|********: 193\n",
      "Epoch 189, loss 0.017889340381678342, epsilon 0.02309082974855046 \n",
      "|replace|: 9\n",
      "Total Reward:  -47.36842105263158 \n",
      "\n",
      "********|engine unit|********: 194\n",
      "Epoch 190, loss 0.04400979322010893, epsilon 0.02309082974855046 \n",
      "|replace|: 4\n",
      "Total Reward:  -48.63945578231292 \n",
      "\n",
      "********|engine unit|********: 195\n",
      "Epoch 191, loss 3336.8271679391214, epsilon 0.02309082974855046 \n",
      "|replace|: 35\n",
      "Total Reward:  -40.74074074074075 \n",
      "\n",
      "********|engine unit|********: 196\n",
      "Epoch 192, loss 522.7501809865445, epsilon 0.02309082974855046 \n",
      "|replace|: 156\n",
      "Total Reward:  -17.22689075630252 \n",
      "\n",
      "********|engine unit|********: 197\n",
      "Epoch 193, loss 0.05481984709707591, epsilon 0.02309082974855046 \n",
      "|cycle reached failure state|: 153 reward: -100 \n",
      "\n",
      "********|engine unit|********: 198\n",
      "Epoch 194, loss 0.008137808196913197, epsilon 0.02309082974855046 \n",
      "|replace|: 204\n",
      "Total Reward:  -100 \n",
      "\n",
      "********|engine unit|********: 199\n",
      "Epoch 195, loss 0.01167551617521763, epsilon 0.02309082974855046 \n",
      "|cycle reached failure state|: 218 reward: -100 \n",
      "\n",
      "********|engine unit|********: 200\n",
      "Epoch 196, loss 0.05366569304472043, epsilon 0.02309082974855046 \n",
      "|replace|: 115\n",
      "Total Reward:  -24.557522123893804 \n",
      "\n",
      "********|engine unit|********: 201\n",
      "Epoch 197, loss 0.040416379564881644, epsilon 0.02309082974855046 \n",
      "|replace|: 106\n",
      "Total Reward:  -22.10526315789474 \n",
      "\n",
      "********|engine unit|********: 202\n",
      "Epoch 198, loss 0.006852075421322496, epsilon 0.02309082974855046 \n",
      "|replace|: 40\n",
      "Total Reward:  -39.795918367346935 \n",
      "\n",
      "********|engine unit|********: 203\n",
      "Epoch 199, loss 0.03261385642551567, epsilon 0.02309082974855046 \n",
      "|replace|: 31\n",
      "Total Reward:  -44.21641791044776 \n",
      "\n",
      "********|engine unit|********: 204\n",
      "Epoch 200, loss 0.02580654578078821, epsilon 0.02309082974855046 \n",
      "|replace|: 26\n",
      "Total Reward:  -44.49152542372881 \n",
      "\n",
      "********|engine unit|********: 205\n",
      "Epoch 201, loss 0.026238399687167898, epsilon 0.0225135590048367 \n",
      "|cycle reached failure state|: 197 reward: -100 \n",
      "\n",
      "********|engine unit|********: 206\n",
      "Epoch 202, loss 0.03914423162014307, epsilon 0.0225135590048367 \n",
      "|replace|: 4\n",
      "Total Reward:  -49.09090909090909 \n",
      "\n",
      "********|engine unit|********: 207\n",
      "Epoch 203, loss 0.05182031432888934, epsilon 0.0225135590048367 \n",
      "|replace|: 83\n",
      "Total Reward:  -27.322404371584696 \n",
      "\n",
      "********|engine unit|********: 208\n",
      "Epoch 204, loss 3351.9272429597017, epsilon 0.0225135590048367 \n",
      "|replace|: 37\n",
      "Total Reward:  -37.58389261744966 \n",
      "\n",
      "********|engine unit|********: 1\n",
      "Epoch 205, loss 459.78760688376525, epsilon 0.0225135590048367 \n"
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "session_rewards=[]\n",
    "epochs=[]\n",
    "losses=[]\n",
    "eps =[]\n",
    "for episode in range(int((df_A['unit'].max() * 80 / 100))):#num_episodes):\n",
    "    while True:\n",
    "        action = get_action(state,epsilon=epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        done = 1 if done else 0\n",
    "        \n",
    "        replay.append((state,action,reward,next_state,done))\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if(action==1):\n",
    "            print(\"Total Reward: \", total_reward,\"\\n\")\n",
    "\n",
    "        if done:\n",
    "            session_rewards.append(total_reward)\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            break\n",
    "\n",
    "    if len(replay)>batch:\n",
    "            batch_ = random.sample(replay,batch)\n",
    "            states = tf.convert_to_tensor([x[0] for x in batch_])\n",
    "            actions = tf.convert_to_tensor([x[1] for x in batch_])\n",
    "            rewards = tf.convert_to_tensor([x[2] for x in batch_])\n",
    "            next_states = tf.convert_to_tensor([x[3] for x in batch_])\n",
    "            done = tf.convert_to_tensor([x[4] for x in batch_])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                #Predicted q-value\n",
    "                predicted_qvalues = network(states)\n",
    "                #kiu = network.predict(states)\n",
    "\n",
    "                predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions, n_actions), axis=1)\n",
    "                predicted_qvalues_for_actions =tf.cast(predicted_qvalues_for_actions, tf.float64)\n",
    "\n",
    "                #Predicted next states' q-value\n",
    "                predicted_next_qvalues = network(next_states)\n",
    "\n",
    "                next_state_values = tf.reduce_max(predicted_next_qvalues,axis=1)\n",
    "                next_state_values = tf.cast(next_state_values, tf.float64)\n",
    "\n",
    "                #target_qvalues_for_actions = rewards + gamma* next_state_values\n",
    "                target_qvalues_for_actions = tf.cast(rewards,tf.float64) + gamma* next_state_values\n",
    "\n",
    "                target_qvalues_for_actions = tf.cast(target_qvalues_for_actions, tf.float64)\n",
    "\n",
    "                done = tf.cast(done, tf.bool)\n",
    "                rewards = tf.cast(rewards, tf.float64)\n",
    "\n",
    "                target_qvalues_for_actions = tf.where(done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "                loss = (tf.stop_gradient(target_qvalues_for_actions) - predicted_qvalues_for_actions) ** 2\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "                grads = tape.gradient(loss, network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "                                \n",
    "                epochs.append(epoch)\n",
    "                losses.append(loss)\n",
    "        \n",
    "                print('Epoch {}, loss {}, epsilon {} '.format(epoch,loss,epsilon))\n",
    "\n",
    "                network.save('rnn.keras')\n",
    "\n",
    "                if epoch > 0:\n",
    "                    if epoch%20==0:\n",
    "                        epsilon*=epsilon_decay\n",
    "                        eps.append(epsilon)\n",
    "                epoch+=1\n",
    "                if epsilon < 0.016: #0.024\n",
    "                    epsilon=initial_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f2a360",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(losses)), losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae4396e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(session_rewards)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(session_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b9d63",
   "metadata": {},
   "source": [
    "policy = pd.DataFrame.from_dict(policy).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acde58a",
   "metadata": {},
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf02ab",
   "metadata": {},
   "source": [
    "result = [item for item in session_rewards if item > -2] \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b62da5",
   "metadata": {},
   "source": [
    "value_network1 = tf.keras.models.load_model('rnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_unit = int((df_A['unit'].max() * 80 / 100) + 1)\n",
    "env = CustomEnv(is_training=False)\n",
    "total_returns_test=[]\n",
    "\n",
    "#session_rewards = [generate_session(epsilon=0, train=False) for _ in range(int(df_A['unit'].max() * 20 / 100))]\n",
    "ttotal_reward=0\n",
    "tsession_rewards=[]\n",
    "#state = env.reset()\n",
    "#print(\"Outside loop\")\n",
    "for episode in range(int(df_A['unit'].max() * 20 / 100)):#num_episodes):\n",
    "    while True:\n",
    "        epsilon = 0.001\n",
    "        value_fn = network.predict(np.array([state]),verbose=0)[0]\n",
    "    \n",
    "        if np.random.rand() < epsilon:\n",
    "            action =np.random.choice(n_actions, 1)[0] \n",
    "        else:\n",
    "            action = np.argmax(value_fn)\n",
    "    \n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        done = 1 if done else 0\n",
    "        \n",
    "        #replay.append((state,action,reward,next_state,done))\n",
    "\n",
    "        ttotal_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #print(\"\\nReplay: \", episode+1)\n",
    "            tsession_rewards.append(ttotal_reward)\n",
    "            print(\"Total Reward: \", ttotal_reward,\"\\n\")\n",
    "            ttotal_reward = 0\n",
    "            state = env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1824da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(tsession_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = pd.DataFrame.from_dict(policy).T\n",
    "policy['remaining_cycles'] = policy['failure_state'] - policy['replace_state']\n",
    "policy_test = pd.DataFrame.from_dict(policy_test).T\n",
    "policy_test['remaining_cycles'] = policy_test['failure_state'] - policy_test['replace_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5712cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_test.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Loss vs. Epoch graph\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(policy['failure_state'] , marker='o', label='Failure State')\n",
    "plt.plot(policy['replace_state'], marker='o', label='Replace State')\n",
    "plt.xlabel('unit')\n",
    "plt.xticks(range(0, 211, 5))\n",
    "plt.ylabel('cycle')\n",
    "plt.title('unit vs. cycle')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0c63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting Testing policy\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(policy_test['failure_state'] , marker='o', label='Failure State')\n",
    "plt.plot(policy_test['replace_state'], marker='o', label='Replace State')\n",
    "plt.xlabel('unit')\n",
    "plt.xticks(range(210, 261, 2))\n",
    "plt.ylabel('cycle')\n",
    "plt.title('unit vs. cycle')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab6726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
